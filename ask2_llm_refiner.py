# ask2_llm_refiner.py — Pure LLM Mode
# - No canned replies: every human-facing message is generated by the LLM.
# - 3-pass self-refinement: draft → critique → validate/compact.
# - Strict mode available (SCAI_LLM_STRICT=1): if LLM unavailable, return machine JSON with empty answer.
# - Restores request body for inner app; friendly JSON shape; deduped Sources footer.

import io, json, re, os
from typing import List, Dict, Any
import urllib.request, urllib.error

def _env(name, default=None):
    return os.environ.get(name, default)

def _openai_chat(messages, model=None, max_tokens=None, temperature=0.2, json_mode=True):
    api_key = _env("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY not set")
    model = model or _env("SCAI_LLM_MODEL", "gpt-4o-mini")
    max_tokens = int(_env("SCAI_LLM_MAX_TOKENS", max_tokens or 700))
    body = {"model": model, "messages": messages, "temperature": float(temperature)}
    if json_mode:
        body["response_format"] = {"type": "json_object"}
    if max_tokens:
        body["max_tokens"] = max_tokens
    req = urllib.request.Request(
        "https://api.openai.com/v1/chat/completions",
        data=json.dumps(body).encode("utf-8"),
        headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
        method="POST"
    )
    with urllib.request.urlopen(req, timeout=60) as resp:
        data = json.loads(resp.read().decode("utf-8"))
    content = data["choices"][0]["message"]["content"]
    if json_mode:
        try: return json.loads(content)
        except Exception: return {"answer": content.strip()}
    return {"answer": content.strip()}

GREET_RE = re.compile(r"^\s*(hi|hello|hey|hola|ol[aá]|oi|howdy|yo)[\s!?.]*$", re.I)
META_RE  = re.compile(r"(who are you|what (is|are) (this|sustaina?core)|what can you do|help|how to use)", re.I)

def _is_greeting(q: str) -> bool: return bool(GREET_RE.match(q or ""))
def _is_meta(q: str) -> bool: return bool(META_RE.search(q or ""))

def _json_or_none(b: bytes):
    try: return json.loads(b.decode("utf-8"))
    except Exception: return None

def _dedup_sources(sources: List[Dict[str, Any]], limit: int = 4) -> List[Dict[str, Any]]:
    seen, out = set(), []
    for s in sources or []:
        title = ""
        url = ""
        if isinstance(s, dict):
            title = (s.get("title") or s.get("name") or s.get("id") or "").strip()
            url = (s.get("url") or s.get("link") or "").strip()
        elif isinstance(s, str):
            title = s.strip()
        key = (title.lower(), url.lower())
        if key in seen: continue
        seen.add(key)
        out.append({"title": title or (url or "Source"), "url": url})
        if len(out) >= limit: break
    return out

def _extract_evidence(payload: Dict[str, Any]) -> Dict[str, Any]:
    ev = {"answer_raw": payload.get("answer", ""), "snippets": [], "sources": []}
    for k in ("snippets","chunks","evidence","fragments","items"):
        v = payload.get(k)
        if isinstance(v, list):
            for it in v:
                if isinstance(it, dict) and it.get("text"):
                    ev["snippets"].append(str(it["text"]))
                elif isinstance(it, dict) and it.get("source"):
                    ev["sources"].append({"title": it["source"], "url": it.get("url","")})
                elif isinstance(it, str):
                    ev["snippets"].append(it)
    if isinstance(payload.get("sources"), list):
        for s in payload["sources"]:
            if isinstance(s, dict):
                ev["sources"].append({"title": s.get("title") or s.get("name") or "", "url": s.get("url") or ""})
            elif isinstance(s, str):
                ev["sources"].append({"title": s, "url": ""})
    return ev

def _final(answer: str, citations: List[Dict[str,str]]):
    return json.dumps({"answer": (answer or "").strip(), "sources": citations}, ensure_ascii=False).encode("utf-8")

class Ask2LLMRefinerMiddleware:
    def __init__(self, app):
        self.app = app
        self.max_sources = int(_env("SCAI_SOURCES_MAX", "4"))
        self.strict = _env("SCAI_LLM_STRICT", "0") == "1"
        # Optional style hint for the LLM (e.g., "concise, friendly, no jargon")
        self.style = _env("SCAI_GUIDE_STYLE", "concise, human, professional; no emojis unless user uses them")

    def __call__(self, environ, start_response):
        if environ.get("PATH_INFO","") != "/ask2" or environ.get("REQUEST_METHOD","GET").upper() == "OPTIONS":
            return self.app(environ, start_response)

        # Buffer body and restore for inner app
        try: length = int(environ.get("CONTENT_LENGTH") or "0")
        except Exception: length = 0
        body = environ["wsgi.input"].read(length) if length > 0 else b"{}"
        req = _json_or_none(body) or {}
        question = (req.get("question") or req.get("q") or "").strip()

        # Greeting/meta: LLM-only. No templates.
        if _is_greeting(question) or _is_meta(question):
            sysmsg = {
                "role":"You are SustainaCore Assistant. Greet briefly; if asked who/what you are, explain in one sentence what SustainaCore/TECH100 is and how to use it. Offer ONE example query relevant to TECH100/ESG. No Sources footer for this.",
                "style": self.style,
                "format":"Return JSON { answer: string }"
            }
            try:
                out = _openai_chat(
                    [{"role":"system","content":json.dumps(sysmsg)},
                     {"role":"user","content":question or "hi"}],
                    json_mode=True
                )
                answer = out.get("answer") or out.get("content") or out.get("text") or ""
            except Exception as e:
                if self.strict:
                    start_response("200 OK", [("Content-Type","application/json; charset=utf-8")])
                    return _final("", [])
                answer = ""  # fall through to minimal empty answer
            start_response("200 OK", [("Content-Type","application/json; charset=utf-8")])
            return [_final(answer, [])]

        # Call inner app (retrieval)
        captured_status = {"v":"200 OK"}
        captured_headers = {"v":[]}
        captured_body = []

        def cap(sr_status, sr_headers, exc_info=None):
            captured_status["v"] = sr_status
            captured_headers["v"] = sr_headers
            return captured_body.append

        inner_env = environ.copy()
        inner_env["wsgi.input"] = io.BytesIO(body)
        inner_env["CONTENT_LENGTH"] = str(len(body))

        try:
            inner_iter = self.app(inner_env, cap)
            for chunk in inner_iter:
                captured_body.append(chunk)
            if hasattr(inner_iter, "close"):
                inner_iter.close()
        except Exception:
            if self.strict:
                start_response("200 OK", [("Content-Type","application/json; charset=utf-8")])
                return [_final("", [])]
            payload = None
        else:
            raw = b"".join(captured_body)
            try:
                payload = json.loads(raw.decode("utf-8"))
            except Exception:
                payload = None

        if not isinstance(payload, dict):
            # No templates: either strict empty or minimal pass-through
            if self.strict:
                start_response("200 OK", [("Content-Type","application/json; charset=utf-8")])
                return [_final("", [])]
            # non-strict: attempt to salvage answer from raw inner output
            start_response("200 OK", [("Content-Type","application/json; charset=utf-8")])
            return [_final("", [])]

        ev = _extract_evidence(payload)
        dedup = _dedup_sources(payload.get("sources") or ev["sources"], limit=self.max_sources)

        # 3-pass LLM refinement (all model-generated)
        sys1 = {
            "role":"You are a meticulous ESG/AI analyst. Use ONLY the provided evidence to answer the user's question. No hallucinations.",
            "style": self.style,
            "format":"Return JSON { answer: string }"
        }
        user1 = {
            "question": question,
            "evidence_snippets": ev["snippets"][:12],
            "notes":[
                "Avoid listing [S1]/[S2].",
                "Be direct in the first sentence. 120-180 words preferred.",
                "If evidence is weak, explain the limitation and suggest 1-2 sharper follow-ups."
            ]
        }
        try:
            draft = _openai_chat(
                [{"role":"system","content":json.dumps(sys1)},
                 {"role":"user","content":json.dumps(user1)}],
                json_mode=True
            )
        except Exception:
            if self.strict:
                start_response("200 OK", [("Content-Type","application/json; charset=utf-8")])
                return [_final("", [])]
            draft = {"answer": ev.get("answer_raw","")}

        sys2 = {
            "role":"You are a senior editor. Critique and improve the draft: remove repetition, tighten, ensure clarity and factual grounding from the evidence.",
            "style": self.style,
            "format":"Return JSON { answer: string }"
        }
        try:
            improved = _openai_chat(
                [{"role":"system","content":json.dumps(sys2)},
                 {"role":"user","content":json.dumps({"question":question,"draft":draft,"evidence_snippets":ev["snippets"][:12]})}], 
                json_mode=True
            )
        except Exception:
            improved = draft

        sys3 = {
            "role":"Validation pass. Ensure all claims are supported by evidence, remove any uncertain statements, and compact the answer. Do not include inline references; we'll add a Sources footer.",
            "style": self.style,
            "format":"Return JSON { answer: string }"
        }
        try:
            final_obj = _openai_chat(
                [{"role":"system","content":json.dumps(sys3)},
                 {"role":"user","content":json.dumps({"question":question,"answer":improved})}], 
                json_mode=True
            )
        except Exception:
            final_obj = improved

        answer = (final_obj.get("answer") or "").strip()
        citations = _dedup_sources(dedup, limit=self.max_sources)

        # Append Sources footer (still LLM-only content above)
        if citations and answer:
            answer += "\n\nSources:\n" + "\n".join("• " + c["title"] + (f" — {c['url']}" if c.get("url") else "") for c in citations)

        start_response("200 OK", [("Content-Type","application/json; charset=utf-8")])
        return [_final(answer, citations)]
